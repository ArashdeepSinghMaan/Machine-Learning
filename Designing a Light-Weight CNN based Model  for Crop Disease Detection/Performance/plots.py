import numpy as np
import matplotlib.pyplot as plt
epochs= range(25)
train_loss=[
        1.4189784241439998,
        0.6152990905872593,
        0.44357485296936683,
        0.36392440530202225,
        0.3077252188433674,
        0.27106863356253885,
        0.24983662101865087,
        0.23006519029615563,
        0.21647095988583484,
        0.19722075718837576,
        0.18442781309741804,
        0.1768265470350456,
        0.16914482985437235,
        0.15949900496039082,
        0.1512206886712234,
        0.14778673805593723,
        0.1443269671955593,
        0.13981364618320963,
        0.13279612619379216,
        0.13073794072929398,
        0.12778274117236252,
        0.12382037613099174,
        0.12010011381140236,
        0.12090176786744872,
        0.11672488953842701
    ]
val_loss =[
        0.5639175199821927,
        0.3610431071351336,
        0.25574405344477447,
        0.17904288487264244,
        0.14686996345941444,
        0.10100674460543035,
        0.09992964034382786,
        0.08330864786128844,
        0.08307758512007685,
        0.11891474101574566,
        0.09728519275238377,
        0.09360192379708424,
        0.05781931417868908,
        0.050695877079465025,
        0.07385627920916937,
        0.07834852416951937,
        0.049718721125901574,
        0.06770292778427285,
        0.05202783242388628,
        0.06271400127132505,
        0.03284270960528928,
        0.034730457575920096,
        0.04696416467004311,
        0.038497918901638326,
        0.028807210098488197
    ]
train_acc= [
        0.5868696208834199,
        0.808777295682481,
        0.8626929369087417,
        0.8844583540792376,
        0.9031083291841525,
        0.9133366526779999,
        0.9209474358062452,
        0.9271783199374066,
        0.9312611138772316,
        0.9369087417312754,
        0.9401664414254215,
        0.9435663987481329,
        0.9459563269080304,
        0.9486165445622021,
        0.9511629561135216,
        0.9521160822249093,
        0.9542783981791024,
        0.9543779785190981,
        0.9576072266875312,
        0.9583611921189275,
        0.9586314816132016,
        0.9597553168788676,
        0.9613059250302298,
        0.9617469236787823,
        0.9622590511416175
    ]
val_acc=[
        0.8211358980195766,
        0.8803209651718644,
        0.9154905531527431,
        0.9450830867288869,
        0.9513999544730253,
        0.968586387434555,
        0.968586387434555,
        0.9713180059185068,
        0.9720578192579103,
        0.9587411791486457,
        0.9661962212610973,
        0.968074208968814,
        0.9810493967675848,
        0.9840086501251992,
        0.9741634418392898,
        0.9736512633735489,
        0.9839517414067835,
        0.9766105167311633,
        0.9824721147279764,
        0.9791145003414523,
        0.9895287958115184,
        0.9880491691327111,
        0.9833257455042113,
        0.986740268609151,
        0.9907807876166629
    ]
precision=[
        0.8352127909510921,
        0.8902667884763761,
        0.9228434868260571,
        0.9479492211459872,
        0.9544758689078695,
        0.969389476189267,
        0.9696422110984931,
        0.9721667686027109,
        0.9724645526677418,
        0.9606896056414183,
        0.9685849271916959,
        0.9695133272306762,
        0.9814309052810807,
        0.9842245611987032,
        0.9754288708209458,
        0.9751699773013873,
        0.9842799517891717,
        0.9774539980901359,
        0.9830407445529967,
        0.9798019738403583,
        0.9896519234679262,
        0.9881550631875006,
        0.9837027927413886,
        0.9868588942689588,
        0.990880132726746
    ]
recall=[
        0.8211358980195766,
        0.8803209651718643,
        0.915490553152743,
        0.9450830867288869,
        0.9513999544730253,
        0.9685863874345549,
        0.9685863874345549,
        0.9713180059185067,
        0.9720578192579103,
        0.9587411791486455,
        0.9661962212610972,
        0.968074208968814,
        0.9810493967675848,
        0.9840086501251992,
        0.9741634418392898,
        0.9736512633735488,
        0.9839517414067835,
        0.9766105167311632,
        0.9824721147279764,
        0.9791145003414523,
        0.9895287958115183,
        0.9880491691327111,
        0.9833257455042113,
        0.9867402686091509,
        0.9907807876166629
    ]
f1_score= [
        0.8192123068997399,
        0.878865174247437,
        0.9151374612474702,
        0.9452196529743463,
        0.950505392347432,
        0.9683790340122831,
        0.9686506551702638,
        0.9713002790781965,
        0.9720514638636304,
        0.9584132278303937,
        0.9661116666359494,
        0.9679162937479711,
        0.981037067218222,
        0.9839806004992826,
        0.9740084617294549,
        0.9737921282297464,
        0.9839209770474657,
        0.9763922783646979,
        0.9824589489701396,
        0.979046417368298,
        0.9895240583211561,
        0.9879904640178401,
        0.983281289337875,
        0.9867066606041919,
        0.9907652304023749
    ]
plt.plot(epochs, val_acc, label="Validation Accuracy", color="black", linestyle="-")
plt.plot(epochs, val_loss, label="Validation Loss", color="blue", linestyle="-")
plt.plot(epochs, train_loss, label="Training Loss", color="orange", linestyle="--")
plt.plot(epochs, train_acc, label="Training Accuracy", color="green", linestyle="-.")
plt.plot(epochs, precision, label="Precision", color="purple", linestyle=":")
plt.plot(epochs, recall, label="Recall", color="red", linestyle="-")
plt.plot(epochs, f1_score, label="F1 Score", color="brown", linestyle="--")

plt.xlabel("Epochs")
plt.ylabel("Value")
plt.legend(loc="best") 
plt.title("Training Metrics over Epochs")


plt.show()